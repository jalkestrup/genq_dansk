{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping of Retsinformation DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "HfFolder.save_token(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape all URLS of vejledninger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_vejledninger_url(url_list):\n",
    "    #Function to extract all unqiue URLs from 1 search page\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url_list)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    soup = soup.find(\"div\", class_=\"search-result-list\")\n",
    "    \n",
    "    urls = set()  # Using a set to store URLs to avoid duplicates\n",
    "    for div in soup.find_all('div', class_='document-entry'):\n",
    "        url = div.get('about')\n",
    "        if url:\n",
    "            urls.add(url)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def make_url_list(n=5):\n",
    "    #Function to loop over N pages of search results and extract all URLs, and return them as a set\n",
    "    #Base URL for search of vejledninger\n",
    "    url_list = 'https://www.retsinformation.dk/documents?dt=180&h=false&page={page}&ps=100&r=30'\n",
    "    \n",
    "    #Url for adding the correct prefix to the URLs (complete address)\n",
    "    url_prefix = 'https://www.retsinformation.dk'\n",
    "    url_set = set()\n",
    "    \n",
    "    #At time of writing, there are 5 pages of results \n",
    "    for page_no in range(0, n):\n",
    "        vejledninger = extract_vejledninger_url(url_list.format(page=page_no))\n",
    "        complete_url = [url_prefix + url for url in vejledninger]\n",
    "        url_set = url_set.union(complete_url)\n",
    "        \n",
    "    return list(url_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "url_list = make_url_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.retsinformation.dk/eli/retsinfo/2019/9607',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2019/9922',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2003/20353',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2021/9596',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2005/10377']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape HTML content of vejledninger (bs object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS**\n",
    "Nogle vejledninger har tilsynedeladende ikke title i \"Titel2\" format, men derimod bare \"Titel' feks tilfældet for pulverlakering vejledning, andre har class TITLE, og andre igen har ingen men blot font size = 5.... Og et par har slet ingen formatering af overskriften i HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def scrape_content(urls):\n",
    "    # Dictionary to store results\n",
    "    driver = webdriver.Chrome()\n",
    "    result_dict = {}\n",
    "\n",
    "    for url in urls:\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load completely\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get the page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        content_div = soup.find(\"div\", class_=\"document-content\")\n",
    "\n",
    "        if content_div:\n",
    "            if content_div.find(\"p\", class_=\"Titel2\"):\n",
    "                title = content_div.find(\"p\", class_=\"Titel2\").get_text(strip=True).replace(\"\\n\", \"\")\n",
    "            elif content_div.find(\"p\", class_=\"Titel\"):\n",
    "                title = content_div.find(\"p\", class_=\"Titel\").get_text(strip=True).replace(\"\\n\", \"\")\n",
    "            elif content_div.find(\"h1\", class_=\"TITLE\"):\n",
    "                title = content_div.find(\"h1\", class_=\"TITLE\").get_text(strip=True).replace(\"\\n\", \"\")\n",
    "            elif content_div.find(\"font\", {\"size\": \"5\"}):\n",
    "                title = content_div.find(\"font\", {\"size\": \"5\"}).get_text(strip=True).replace(\"\\n\", \"\")\n",
    "            else:\n",
    "                title = str(url)\n",
    "                \n",
    "            result_dict[title] = content_div\n",
    "        else:\n",
    "            print(\"Content not found for URL:\", url)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note, scraping mighte take several hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "vejledninger_raw = scrape_content(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For brief testing, take first 5 URLS and print**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Arbejde med flyveaske', 'Vejledning om regulering af satser fra 1. januar 2020 efter lov om arbejdsskadesikring, lov om sikring mod følger af arbejdsskade, lov om arbejdsskadeforsikring og lov om forsikring mod følger af ulykkestilfælde', 'Flugtveje   og sikkerhedsbelysning (nødbelysning) på faste arbejdssteder - At-vejledning   A.1.10 - December 2003 – Erstatter At-meddelelse nr. 1.01.2 af januar   1989', 'At-vejledning 13.0.1-1 om undervisningspligtige unges arbejde', 'Arbejdsrelateret muskel- og skeletbesvær - At-vejledning D.3.4 - Maj 2005'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vejledninger_raw = scrape_content(url_list[0:5])\n",
    "#vejledninger_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arbejde med flyveaskeSundhedsfarer og forebyggelse ved arbejde med flyveaske.At-vejledning D.2.21-11. februar - Opdateret juli 2019Erstatter At-meddelelse nr. 4.04.17 af oktober 1990Denne vejledning oplyser om de sundhedsfarer, der er forbundet med arbejde med flyveaske, og om, hvilke foranstaltninger der skal træffes for at imødegå dem.Flyveaske anvendes som fyldmateriale i forbindelse med vejbygning, i cement- og betonindustrien og ved produktion af gasbeton.SundhedsfareFlyveaske består af finkornede partikler, der udskilles af røggasserne fra kulfyrede kraftværker. Flyveaske er et variabelt produkt, hvis egenskaber og kemiske sammensætning afhænger af de anvendte kul, den anvendte forbrændingsteknik og røgrensningsteknikken.Flyveaske indeholder spormængder af bl.a. en række tungmetaller. Indholdet af krystallinsk siliciumdioxid, herunder α-kvarts, kan være over 0,1 pct., hvorfor unge under 18 år ikke må arbejde med flyveaske.Da asken er basisk, kan der ske irritation af hud og slimh'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vejledninger_raw['Arbejde med flyveaske'].get_text(strip=True).replace(\"\\n\", \"\")[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_text(bs_obj):\n",
    "    # Find all <p> tags and get their text content\n",
    "    paragraphs = bs_obj.find_all(\"p\")\n",
    "    \n",
    "    # Concatenate the text content of each <p> tag, separated by a line break\n",
    "    text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace two or more line breaks in a row with a double line break\n",
    "    cleaned_text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def create_vejl_dict(vejledninger_raw):\n",
    "    #vejledninger = scrape_content(urls)\n",
    "    vejledninger_dict = {}\n",
    "    for title, bs_obj in vejledninger_raw.items():\n",
    "        text = extract_text(bs_obj)\n",
    "        cleaned_text = clean_text(text)\n",
    "        vejledninger_dict[title] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "vejledninger_text = create_vejl_dict(vejledninger_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Loading previous CSV temporarily in order not to have to scrape again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload vejledninger_html_2-11-2023.csv into vejledninger_html_dict as key,value pairs\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "vejledninger_tekst_dict = {}\n",
    "csv_file_path = \"data/data/vejledninger_tekst.csv\"\n",
    "with open(csv_file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    # Skip first row\n",
    "    next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "        value = row[1]\n",
    "        vejledninger_tekst_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description=\"# Vejledninger fra Retsinformation.dk\\nDatasættet indeholder alle vejledninger scrapet fra Retsinformation.dk (pr. November 2023).\\nDatasættet indeholder 2 kolonner: navnet på den givne vejledning (såfremt det fremgik af html'en, ellers fremgår URL) og hele indholdet af vejledningen.\\nTeksten er renset og formateret således at der er 1 linjeskift (\\n) mellem hver sektion ( <p> tag), \\nmedmindre der er indsat en eller flere tomme sektioner i træk hvormed der i stedet er indsat 2 linjeskift (\\n\\n)\\n\", citation='', homepage='', license='', features={'vejledning': Value(dtype='string', id=None), 'indhold': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, dataset_name='Vejledninger fra Retsinformation.dk', config_name='retsinformation', version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create hugging face dataset using the _from_dict method\n",
    "\n",
    "from datasets import Dataset\n",
    "vejledning_navn = list(vejledninger_tekst_dict.keys())\n",
    "vejledning_tekst = list(vejledninger_tekst_dict.values())\n",
    "\n",
    "ds_vejledning = Dataset.from_dict({\"vejledning\": vejledning_navn, \"indhold\": vejledning_tekst})\n",
    "\n",
    "# add readme\n",
    "ds_vejledning.info.description = \"\"\"# Vejledninger fra Retsinformation.dk\n",
    "Datasættet indeholder alle vejledninger scrapet fra Retsinformation.dk (pr. November 2023).\n",
    "Datasættet indeholder 2 kolonner: navnet på den givne vejledning (såfremt det fremgik af html'en, ellers fremgår URL) og hele indholdet af vejledningen.\n",
    "Teksten er renset og formateret således at der er 1 linjeskift (\\n) mellem hver sektion ( <p> tag), \n",
    "medmindre der er indsat en eller flere tomme sektioner i træk hvormed der i stedet er indsat 2 linjeskift (\\n\\n)\n",
    "\"\"\"\n",
    "\n",
    "ds_vejledning.info.dataset_name = \"Vejledninger fra Retsinformation.dk\"\n",
    "ds_vejledning.info.config_name = \"data-retsinformation\"\n",
    "\n",
    "ds_vejledning.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4c4599bb78404d85555974e56ae43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e362a9efece4a76a19f30ab51d8f6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8780fb91a0d4ae78a122a6b083da4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload dataset\n",
    "ds_vejledning.push_to_hub(repo_id=\"dk_retrieval_benchmark\", config_name=\"retsinformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
