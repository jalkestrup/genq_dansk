{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping of Retsinformation.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "HfFolder.save_token(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape all URLS of vejledninger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_urls_from_page(url):\n",
    "    \"\"\"\n",
    "    Extract all unique URLs from a single search page.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of extracted URLs.\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    with webdriver.Chrome(options=options) as driver:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the specific element to be loaded\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"document-entry\"))\n",
    "        )\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        search_results = soup.find(\"div\", class_=\"search-result-list\")\n",
    "\n",
    "        urls = []\n",
    "        for div in search_results.find_all('div', class_='document-entry'):\n",
    "            url = div.get('about')\n",
    "            if url:\n",
    "                urls.append(url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def make_url_list(n=5, url_prefix='https://www.retsinformation.dk'):\n",
    "    \"\"\"\n",
    "    Function to loop over N pages of search results and extract all URLs.\n",
    "\n",
    "    Args:\n",
    "    n (int): Number of pages to scrape.\n",
    "    url_prefix (str): Prefix to complete the URLs.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of all extracted and complete URLs.\n",
    "    \"\"\"\n",
    "    base_url = f'{url_prefix}/documents?dt=180&h=false&page={{page}}&ps=100&r=30'\n",
    "    all_urls = []\n",
    "\n",
    "    for page_no in range(0, n):\n",
    "        full_url = base_url.format(page=page_no)\n",
    "        page_urls = extract_urls_from_page(full_url)\n",
    "        complete_urls = [url_prefix + url for url in page_urls]\n",
    "        all_urls.extend(complete_urls)\n",
    "\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.retsinformation.dk/eli/retsinfo/2024/9001',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2024/9000',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2023/10095',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2023/10093',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2023/10092']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Took 10 seconds to run\n",
    "url_list = make_url_list()\n",
    "\n",
    "#Return first 5 URLs of the list\n",
    "url_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape HTML content of vejledninger (bs object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS**\n",
    "Nogle vejledninger har tilsynedeladende ikke title i \"Titel2\" format, men derimod bare \"Titel' feks tilfældet for pulverlakering vejledning, andre har class TITLE, og andre igen har ingen men blot font size = 5.... Og et par har slet ingen formatering af overskriften i HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(content_div):\n",
    "    \"\"\"\n",
    "    Extracts the title from a given BeautifulSoup div element.\n",
    "\n",
    "    This function searches for the title of the content in the provided div element. It checks for various HTML tags and classes to find the title. If none of the specified tags and classes are found, it returns None.\n",
    "\n",
    "    Args:\n",
    "        content_div (BeautifulSoup element): The BeautifulSoup element representing a div from which the title is to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted title as a string, or None if no title is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    title_elements = [\n",
    "        {\"tag\": \"p\", \"class_\": \"Titel2\"},\n",
    "        {\"tag\": \"p\", \"class_\": \"Titel\"},\n",
    "        {\"tag\": \"h1\", \"class_\": \"TITLE\"},\n",
    "        {\"tag\": \"font\", \"attrs\": {\"size\": \"5\"}}\n",
    "    ]\n",
    "\n",
    "    for elem in title_elements:\n",
    "        if \"attrs\" in elem:\n",
    "            title = content_div.find(elem[\"tag\"], **elem[\"attrs\"])\n",
    "        else:\n",
    "            title = content_div.find(elem[\"tag\"], class_=elem[\"class_\"])\n",
    "        \n",
    "        if title:\n",
    "            return title.get_text(strip=True).replace(\"\\n\", \"\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_content(urls):\n",
    "    \"\"\"\n",
    "    Scrapes and collects HTML content from a given list of URLs.\n",
    "\n",
    "    This function navigates to each URL, waits for the page's content to load, and then extracts  HTML content within the \"document-content\" div using BS. \n",
    "    It attempts to identify and use the title of the content key in the resulting dictionary.\n",
    "\n",
    "    Args:\n",
    "        urls (list of str): A list of URLs to be scraped.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with titles as keys and the corresponding HTML content as values. If the title can't be determined, the URL is used as the key.\n",
    "    \"\"\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    result_dict = {}\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the specific element to be loaded\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"document-content\"))\n",
    "            )\n",
    "\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            content_div = soup.find(\"div\", class_=\"document-content\")\n",
    "\n",
    "            if content_div:\n",
    "                title = extract_title(content_div) or str(url)\n",
    "                result_dict[title] = content_div\n",
    "            else:\n",
    "                print(\"Content not found for URL:\", url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {url}: {e}\")\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Took 6 minutes to run\n",
    "vejledninger_raw = scrape_content(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arbejde med flyveaskeSundhedsfarer og forebyggelse ved arbejde med flyveaske.At-vejledning D.2.21-11. februar - Opdateret juli 2019Erstatter At-meddelelse nr. 4.04.17 af oktober 1990Denne vejledning oplyser om de sundhedsfarer, der er forbundet med arbejde med flyveaske, og om, hvilke foranstaltninger der skal træffes for at imødegå dem.Flyveaske anvendes som fyldmateriale i forbindelse med vejbygning, i cement- og betonindustrien og ved produktion af gasbeton.SundhedsfareFlyveaske består af finkornede partikler, der udskilles af røggasserne fra kulfyrede kraftværker. Flyveaske er et variabelt produkt, hvis egenskaber og kemiske sammensætning afhænger af de anvendte kul, den anvendte forbrændingsteknik og røgrensningsteknikken.Flyveaske indeholder spormængder af bl.a. en række tungmetaller. Indholdet af krystallinsk siliciumdioxid, herunder α-kvarts, kan være over 0,1 pct., hvorfor unge under 18 år ikke må arbejde med flyveaske.Da asken er basisk, kan der ske irritation af hud og slimh'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect one of the scraped documents\n",
    "vejledninger_raw['Arbejde med flyveaske'].get_text(strip=True).replace(\"\\n\", \"\")[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(bs_obj):\n",
    "    \"\"\"\n",
    "    Extracts and concatenates text content from all <p> tags in the provided BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        bs_obj (BeautifulSoup): A BeautifulSoup object representing parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing all text from <p> tags, separated by line breaks.\n",
    "    \"\"\"\n",
    "    paragraphs = bs_obj.find_all(\"p\")\n",
    "    text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the given text by replacing sequences of two or more line breaks with a double line break.\n",
    "    Function intended for adding further text cleaning steps if needed.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with standardized line breaks.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    cleaned_text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def create_vejl_dict(vejledninger_raw):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of cleaned text for each item in the vejledninger data.\n",
    "\n",
    "    This function iterates over the vejledninger_raw dictionary, extracts and cleans the text for each item, and stores it in a new dictionary with the same keys.\n",
    "\n",
    "    Args:\n",
    "        vejledninger_raw (dict): A dictionary with titles as keys and BeautifulSoup objects as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with titles as keys and cleaned text as values.\n",
    "    \"\"\"\n",
    "    vejledninger_dict = {}\n",
    "    for title, bs_obj in vejledninger_raw.items():\n",
    "        text = extract_text(bs_obj)\n",
    "        cleaned_text = clean_text(text)\n",
    "        vejledninger_dict[title] = cleaned_text\n",
    "    return vejledninger_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vejledninger_text = create_vejl_dict(vejledninger_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vejledning_navn = list(vejledninger_text.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vejledning om regulering af satser fra 1. januar 2024 efter lov om arbejdsskadesikring, lov om sikring mod følger af arbejdsskade, lov om arbejdsskadeforsikring og lov om forsikring mod følger af ulykkestilfælde',\n",
       " 'Vejledning om satser i 2024 for betaling af udgifter til transport m.v. i forbindelse med lægebehandling, der er begæret af Arbejdsmarkedets Erhvervssikring eller Ankestyrelsen',\n",
       " 'Vejledning om obligatorisk selvbooking af jobsamtaler for forskellige målgrupper',\n",
       " 'Vejledning til bekendtgørelse om tilskud til selvstændigt erhvervsdrivende med nedsat arbejdsevne og ansættelse som lønmodtager i fleksjob i en ægtefælles virksomhed',\n",
       " 'Vejledning om fleksløntilskud m.v.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vejledning_navn[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.retsinformation.dk/eli/retsinfo/2024/9001',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2024/9000',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2023/10095',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2023/10093',\n",
       " 'https://www.retsinformation.dk/eli/retsinfo/2023/10092']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Loading previous CSV temporarily in order not to have to scrape again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description=\"# Vejledninger fra Retsinformation.dk\\nDatasættet indeholder alle vejledninger scrapet fra Retsinformation.dk (pr. November 2023).\\nDatasættet indeholder 2 kolonner: navnet på den givne vejledning (såfremt det fremgik af html'en, ellers fremgår URL) og hele indholdet af vejledningen.\\nTeksten er renset og formateret således at der er 1 linjeskift (\\n) mellem hver sektion ( <p> tag), \\nmedmindre der er indsat en eller flere tomme sektioner i træk hvormed der i stedet er indsat 2 linjeskift (\\n\\n)\\n\", citation='', homepage='', license='', features={'vejledning': Value(dtype='string', id=None), 'indhold': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, dataset_name='Vejledninger fra Retsinformation.dk', config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create hugging face dataset using the _from_dict method\n",
    "\n",
    "from datasets import Dataset\n",
    "vejledning_navn = list(vejledninger_text.keys())\n",
    "vejledning_tekst = list(vejledninger_text.values())\n",
    "vejledning_url = url_list\n",
    "\n",
    "\n",
    "ds_vejledning = Dataset.from_dict({\"vejledning\": vejledning_navn, \"indhold\": vejledning_tekst, \"url\": vejledning_url})\n",
    "\n",
    "# add readme\n",
    "ds_vejledning.info.description = \"\"\"# Vejledninger fra Retsinformation.dk\n",
    "Datasættet indeholder alle vejledninger scrapet fra Retsinformation.dk (pr. November 2023).\n",
    "Datasættet indeholder 2 kolonner: navnet på den givne vejledning (såfremt det fremgik af html'en, ellers fremgår URL) og hele indholdet af vejledningen.\n",
    "Teksten er renset og formateret således at der er 1 linjeskift (\\n) mellem hver sektion ( <p> tag), \n",
    "medmindre der er indsat en eller flere tomme sektioner i træk hvormed der i stedet er indsat 2 linjeskift (\\n\\n)\n",
    "\"\"\"\n",
    "\n",
    "ds_vejledning.info.dataset_name = \"Vejledninger fra Retsinformation.dk\"\n",
    "#ds_vejledning.info.config_name = \"retsinformation\"\n",
    "\n",
    "ds_vejledning.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8597360f509642e29bfc60cf8a488c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9cd8cc8baa42b690cc84a53c03084b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f49797d2104d85af4a4ab620be5e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload dataset\n",
    "#Transform to DatasetDict\n",
    "from datasets import DatasetDict\n",
    "ds_dict = DatasetDict({\"train\": ds_vejledning})\n",
    "ds_dict.push_to_hub(repo_id='dk_retrieval_benchmark', config_name='retsinformation')\n",
    "#ds_vejledning.push_to_hub(repo_id=\"dk_retrieval_benchmark\", config_name=\"retsinformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
